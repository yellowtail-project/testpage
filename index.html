<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>iPad Safari - 3秒録画 + MediaPipe Blendshapes</title>
  <style>
    body { font-family: system-ui, -apple-system, sans-serif; margin: 16px; }
    .row { display: flex; gap: 12px; flex-wrap: wrap; align-items: center; }
    button { padding: 10px 14px; font-size: 16px; }
    video { width: 320px; max-width: 100%; background: #000; border-radius: 8px; }
    canvas { width: 320px; max-width: 100%; border: 1px solid #ddd; border-radius: 8px; }
    pre { white-space: pre-wrap; background: #f6f6f6; padding: 12px; border-radius: 8px; max-width: 680px; }
    .hint { color: #555; font-size: 13px; }
  </style>
</head>
<body>
  <h2>0952_3秒録画 → 各フレームでFace Blendshapes表示（MediaPipe）</h2>

  <div class="row">
    <button id="btnCam">カメラ起動（インカメ）</button>
    <button id="btnRec" disabled>撮影開始（3秒）</button>
  </div>
  <p class="hint">
    ※HTTPSで配信してください（例：ローカルなら簡易サーバー）。ボタン操作後にカメラ許可が出ます。
  </p>

  <div class="row" style="margin-top: 12px;">
    <div>
      <div class="hint">プレビュー（カメラ）</div>
      <video id="preview" playsinline autoplay muted></video>
    </div>

    <div>
      <div class="hint">解析用（録画Blob再生）</div>
      <video id="playback" playsinline controls></video>
    </div>

    <div>
      <div class="hint">解析フレーム（canvas）</div>
      <canvas id="canvas" width="320" height="240"></canvas>
    </div>
  </div>

  <h3>Blendshapes（各フレーム）</h3>
  <pre id="out">未解析</pre>

  <script type="module">
  import { FilesetResolver, FaceLandmarker } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.21";

  const btnCam = document.getElementById("btnCam");
  const btnRec = document.getElementById("btnRec");
  const preview = document.getElementById("preview");
  const playback = document.getElementById("playback");
  const canvas = document.getElementById("canvas");
  const ctx = canvas.getContext("2d");
  const out = document.getElementById("out");

  let stream = null;
  let recorder = null;
  let chunks = [];
  let faceLandmarker = null;

  // 追加：全フレーム分ログを保持
  let logs = [];
  let frameIndex = 0;

  // MediaPipe FaceLandmarker 初期化
  async function initMediapipe() {
    if (faceLandmarker) return faceLandmarker;

    const fileset = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.21/wasm"
    );

    faceLandmarker = await FaceLandmarker.createFromOptions(fileset, {
      baseOptions: {
        modelAssetPath:
          "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
        delegate: "GPU"
      },
      runningMode: "VIDEO",
      outputFaceBlendshapes: true,
      numFaces: 1
    });

    return faceLandmarker;
  }

  // インカメ起動
  btnCam.addEventListener("click", async () => {
    out.textContent = "カメラ起動中...";
    try {
      stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: "user",
          width: { ideal: 1280 },
          height: { ideal: 720 }
        },
        audio: false
      });
      preview.srcObject = stream;

      await initMediapipe();

      btnRec.disabled = false;
      out.textContent = "カメラ起動OK。次に「撮影開始（3秒）」を押してください。";
    } catch (e) {
      console.error(e);
      out.textContent = "カメラ起動に失敗: " + e.message;
    }
  });

  // 3秒録画
  btnRec.addEventListener("click", async () => {
    if (!stream) return;

    out.textContent = "録画開始...";
    chunks = [];

    try {
      recorder = new MediaRecorder(stream);
    } catch (e) {
      out.textContent = "MediaRecorderが使えない/初期化できない: " + e.message;
      return;
    }

    recorder.ondataavailable = (ev) => {
      if (ev.data && ev.data.size > 0) chunks.push(ev.data);
    };

    recorder.onstop = async () => {
      const blob = new Blob(chunks, { type: recorder.mimeType || "video/webm" });

      // 追加：解析ログ初期化
      logs = [];
      frameIndex = 0;
      out.textContent = `録画完了。Blobサイズ: ${blob.size.toLocaleString()} bytes\n解析中...\n`;

      // 録画Blobを再生用videoへ
      const url = URL.createObjectURL(blob);
      playback.src = url;
      playback.currentTime = 0;

      // メタデータ読み込み待ち
      await new Promise((res) => {
        if (playback.readyState >= 1) return res();
        playback.onloadedmetadata = () => res();
      });

      // 解析開始（フレームごと）
      await analyzeVideoFrames();

      out.textContent += "\n\n解析完了。";
    };

    recorder.start();

    // 3秒で停止
    setTimeout(() => {
      if (recorder && recorder.state !== "inactive") recorder.stop();
    }, 3000);
  });

  // フレームごとにblendshapes推論して表示
  async function analyzeVideoFrames() {
    const landmarker = await initMediapipe();

    // 再生開始
    await playback.play();

    // iOS Safari: requestVideoFrameCallback が使えればそれを優先
    if ("requestVideoFrameCallback" in HTMLVideoElement.prototype) {
      await new Promise((resolve) => {
        const step = async (now, metadata) => {
          if (playback.ended || playback.paused) {
            resolve();
            return;
          }
          await processOneFrame(landmarker, metadata.mediaTime * 1000);
          playback.requestVideoFrameCallback(step);
        };
        playback.requestVideoFrameCallback(step);
      });
    } else {
      // フォールバック: タイマーで疑似的にフレーム処理
      await new Promise((resolve) => {
        const timer = setInterval(async () => {
          if (playback.ended || playback.paused) {
            clearInterval(timer);
            resolve();
            return;
          }
          await processOneFrame(landmarker, playback.currentTime * 1000);
        }, 1000 / 30);
      });
    }
  }

  // 変更：毎フレームの結果を「上書き」ではなく「蓄積して全表示」
  async function processOneFrame(landmarker, timestampMs) {
    // video → canvas
    const vw = playback.videoWidth || 320;
    const vh = playback.videoHeight || 240;
    canvas.width = 320;
    canvas.height = Math.round(320 * (vh / vw));
    ctx.drawImage(playback, 0, 0, canvas.width, canvas.height);

    // 推論（VIDEOモード）
    const result = landmarker.detectForVideo(playback, timestampMs);

    let text = `frame=${frameIndex}, t=${(timestampMs / 1000).toFixed(3)}s\n`;

    const face = result.faceBlendshapes?.[0];
    if (!face) {
      text += "  顔検出なし\n";
    } else {
      const top = [...face.categories]
        .sort((a, b) => b.score - a.score)
        .slice(0, 12);

      for (const c of top) {
        text += `  ${c.categoryName}: ${c.score.toFixed(3)}\n`;
      }
    }

    logs.push(text);
    frameIndex++;

    // 全ログを画面に出す（シンプル版：毎フレーム更新）
    out.textContent = logs.join("\n");
  }
</script>

</body>
</html>
